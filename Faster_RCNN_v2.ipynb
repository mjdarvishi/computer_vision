{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "B4-HEUo8YeBS"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQQ1vZgTL15E",
        "outputId": "f8e33004-7557-4052-969d-4bc579b267b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "COCO format annotations saved to ships-in-aerial-images/ships-aerial-images\\train\\annotations_coco\\annotations_coco.json\n",
            "COCO format annotations saved to ships-in-aerial-images/ships-aerial-images\\valid\\annotations_coco\\annotations_coco.json\n",
            "COCO format annotations saved to ships-in-aerial-images/ships-aerial-images\\test\\annotations_coco\\annotations_coco.json\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import cv2\n",
        "\n",
        "# Define your category mapping\n",
        "categories = [\n",
        "    {\"id\": 1, \"name\": \"ship\"}\n",
        "]\n",
        "\n",
        "def create_coco_annotation(ann_id, image_id, width, height, annotations):\n",
        "    coco_annotations = []\n",
        "    for ann in annotations:\n",
        "        coco_annotation = {\n",
        "            \"id\": ann_id,\n",
        "            \"image_id\": image_id,\n",
        "            \"category_id\": ann['category_id'],\n",
        "            \"bbox\": ann['bbox'],\n",
        "            \"area\": ann['bbox'][2] * ann['bbox'][3],\n",
        "            \"iscrowd\": 0\n",
        "        }\n",
        "        coco_annotations.append(coco_annotation)\n",
        "        ann_id += 1\n",
        "    return coco_annotations, ann_id\n",
        "\n",
        "def create_coco_image(image_id, file_name, width, height):\n",
        "    return {\n",
        "        \"id\": image_id,\n",
        "        \"file_name\": file_name,\n",
        "        \"width\": width,\n",
        "        \"height\": height\n",
        "    }\n",
        "\n",
        "# Function to process a single dataset (train, valid, or test)\n",
        "def convert_dataset_to_coco(dataset_path):\n",
        "    images_path = os.path.join(dataset_path, \"images\")\n",
        "    annotations_path = os.path.join(dataset_path, \"labels\")\n",
        "    output_path = os.path.join(dataset_path, \"annotations_coco\")\n",
        "\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "    images = []\n",
        "    annotations = []\n",
        "    annotation_id = 1\n",
        "\n",
        "    for image_filename in os.listdir(images_path):\n",
        "        if not image_filename.endswith(\".jpg\"):\n",
        "            continue\n",
        "\n",
        "        image_id = os.path.splitext(image_filename)[0]\n",
        "        image_path = os.path.join(images_path, image_filename)\n",
        "        annotation_path = os.path.join(annotations_path, f\"{image_id}.txt\")\n",
        "\n",
        "        # Error handling for image loading\n",
        "        try:\n",
        "            image = cv2.imread(image_path)\n",
        "            if image is None:  # Check if image loaded successfully\n",
        "                print(f\"Error loading image: {image_path}\")\n",
        "                continue\n",
        "            height, width, _ = image.shape\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image {image_path}: {e}\")\n",
        "            continue\n",
        "\n",
        "        images.append(create_coco_image(image_id, image_filename, width, height))\n",
        "\n",
        "        image_annotations = []\n",
        "        if os.path.exists(annotation_path) and os.path.getsize(annotation_path) > 0:\n",
        "            with open(annotation_path, \"r\") as file:\n",
        "                for line in file:\n",
        "                    parts = line.strip().split()\n",
        "                    yolo_class_id = int(parts[0])\n",
        "                    x_center = float(parts[1]) * width\n",
        "                    y_center = float(parts[2]) * height\n",
        "                    bbox_width = float(parts[3]) * width\n",
        "                    bbox_height = float(parts[4]) * height\n",
        "\n",
        "                    # Convert YOLO bbox format to COCO bbox format\n",
        "                    x_min = x_center - bbox_width / 2\n",
        "                    y_min = y_center - bbox_height / 2\n",
        "\n",
        "                    image_annotations.append({\n",
        "                        \"category_id\": yolo_class_id + 1,  # COCO category IDs are 1-based\n",
        "                        \"bbox\": [x_min, y_min, bbox_width, bbox_height]\n",
        "                    })\n",
        "\n",
        "        if image_annotations:\n",
        "            coco_image_annotations, annotation_id = create_coco_annotation(\n",
        "                annotation_id, image_id, width, height, image_annotations\n",
        "            )\n",
        "            annotations.extend(coco_image_annotations)\n",
        "\n",
        "    coco_format = {\n",
        "        \"images\": images,\n",
        "        \"annotations\": annotations,\n",
        "        \"categories\": categories\n",
        "    }\n",
        "\n",
        "    output_json_path = os.path.join(output_path, \"annotations_coco.json\")\n",
        "    with open(output_json_path, \"w\") as json_file:\n",
        "        json.dump(coco_format, json_file, indent=4)\n",
        "\n",
        "    print(f\"COCO format annotations saved to {output_json_path}\")\n",
        "\n",
        "\n",
        "dataset_base_path = \"ships-in-aerial-images/ships-aerial-images\"\n",
        "for dataset_type in [\"train\", \"valid\", \"test\"]:\n",
        "  dataset_path = os.path.join(dataset_base_path, dataset_type)\n",
        "  convert_dataset_to_coco(dataset_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Xczhgd4aLtnI"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Utente\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision.transforms import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "class ShipDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, transforms=None):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"images\"))))\n",
        "\n",
        "        # Load COCO annotations\n",
        "        with open(os.path.join(root, \"annotations_coco\", \"annotations_coco.json\"), 'r') as f:\n",
        "            self.coco_annotations = json.load(f)\n",
        "\n",
        "        # Create a mapping from image ID to annotations\n",
        "        self.img_id_to_annotations = {}\n",
        "        for ann in self.coco_annotations[\"annotations\"]:\n",
        "            img_id = ann[\"image_id\"]\n",
        "            if img_id not in self.img_id_to_annotations:\n",
        "                self.img_id_to_annotations[img_id] = []\n",
        "            self.img_id_to_annotations[img_id].append(ann)\n",
        "\n",
        "        # Create a mapping from image ID to image info\n",
        "        self.img_id_to_info = {img[\"id\"]: img for img in self.coco_annotations[\"images\"]}\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      img_filename = self.imgs[idx]\n",
        "      img_id = os.path.splitext(img_filename)[0]\n",
        "\n",
        "      if img_id not in self.img_id_to_annotations:\n",
        "          return None, None\n",
        "\n",
        "      img_info = self.img_id_to_info[img_id]\n",
        "      img_path = os.path.join(self.root, \"images\", img_filename)\n",
        "\n",
        "      try:\n",
        "          img = Image.open(img_path).convert(\"RGB\")\n",
        "      except IOError:\n",
        "          print(f\"Error loading image: {img_path}\")\n",
        "          return None, None  # Return None for both image and target\n",
        "\n",
        "      annotations = self.img_id_to_annotations[img_id]\n",
        "\n",
        "      boxes = []\n",
        "      labels = []\n",
        "\n",
        "      for ann in annotations:\n",
        "          labels.append(ann[\"category_id\"])\n",
        "          bbox = ann[\"bbox\"]\n",
        "          xmin = bbox[0]\n",
        "          ymin = bbox[1]\n",
        "          xmax = bbox[0] + bbox[2]\n",
        "          ymax = bbox[1] + bbox[3]\n",
        "\n",
        "          # Check if the bounding box has positive height and width\n",
        "          if xmax > xmin and ymax > ymin:\n",
        "              boxes.append([xmin, ymin, xmax, ymax])\n",
        "          else:\n",
        "              # Skip invalid bounding boxes\n",
        "              continue\n",
        "\n",
        "      if not boxes:\n",
        "          # If no valid bounding boxes are found, return None\n",
        "          return None, None\n",
        "\n",
        "      boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "      labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "\n",
        "      image_id = torch.tensor([idx])\n",
        "      area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "      iscrowd = torch.zeros((len(boxes),), dtype=torch.int64)\n",
        "\n",
        "      target = {\n",
        "          \"boxes\": boxes,\n",
        "          \"labels\": labels,\n",
        "          \"image_id\": image_id,\n",
        "          \"area\": area,\n",
        "          \"iscrowd\": iscrowd\n",
        "      }\n",
        "\n",
        "      if self.transforms is not None:\n",
        "          img = self.transforms(img)\n",
        "\n",
        "      return img, target\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nu1nQwk2LojN",
        "outputId": "5510cdb2-5a9d-40c7-dc85-ccf13a405ada"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Utente\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\Utente\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to C:\\Users\\Utente/.cache\\torch\\hub\\checkpoints\\fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
            "100%|██████████| 160M/160M [00:18<00:00, 9.30MB/s] \n"
          ]
        }
      ],
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "def get_model(num_classes):\n",
        "    # Load a model pre-trained on COCO\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "    # Get the number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "\n",
        "    # Replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Our dataset has one class (ship) + background\n",
        "num_classes = 2\n",
        "model = get_model(num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MX-GSpNEQMsg",
        "outputId": "287f0f28-de9a-4837-bc09-404c8531bdf3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/1:  29%|██▉       | 349/1213 [1:53:21<3:33:19, 14.81s/batch, loss=0.309]"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as T\n",
        "# Define transformations\n",
        "transform = T.Compose([T.ToTensor()])\n",
        "\n",
        "# Create the dataset and dataloader\n",
        "dataset = ShipDataset(f\"{dataset_base_path}/train\", transforms=transform)\n",
        "data_loader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
        "\n",
        "# Move model to the right device\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Construct an optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "# Learning rate scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "\n",
        "# Number of epochs\n",
        "num_epochs = 1\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    progress_bar = tqdm(data_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
        "\n",
        "    for i, (images, targets) in enumerate(progress_bar):\n",
        "        images = list(image.to(device) for image in images if image is not None)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets if t is not None]\n",
        "\n",
        "        # Forward pass\n",
        "        loss_dict = model(images, targets)\n",
        "\n",
        "        # Compute losses\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update running loss\n",
        "        running_loss += losses.item()\n",
        "\n",
        "        # Update progress bar\n",
        "        progress_bar.set_postfix(loss=running_loss/(i+1))\n",
        "\n",
        "    # Update the learning rate\n",
        "    lr_scheduler.step()\n",
        "torch.save(model.state_dict(), 'ship_detection_model.pth')\n",
        "print(\"Model saved to 'ship_detection_model.pth'\")\n",
        "print(\"Training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "nySLeAO9W-1V"
      },
      "outputs": [],
      "source": [
        "# Function to calculate IoU (Intersection over Union)\n",
        "def calculate_iou(box1, box2):\n",
        "    \"\"\"Calculates Intersection over Union (IoU) for two bounding boxes.\n",
        "\n",
        "    Args:\n",
        "        box1: [xmin, ymin, xmax, ymax]\n",
        "        box2: [xmin, ymin, xmax, ymax]\n",
        "\n",
        "    Returns:\n",
        "        iou: IoU value (float)\n",
        "    \"\"\"\n",
        "    x1 = max(box1[0], box2[0])\n",
        "    y1 = max(box1[1], box2[1])\n",
        "    x2 = min(box1[2], box2[2])\n",
        "    y2 = min(box1[3], box2[3])\n",
        "\n",
        "    # Calculate area of intersection\n",
        "    intersection_area = max(0, x2 - x1 + 1) * max(0, y2 - y1 + 1)\n",
        "\n",
        "    # Calculate area of both bounding boxes\n",
        "    box1_area = (box1[2] - box1[0] + 1) * (box1[3] - box1[1] + 1)\n",
        "    box2_area = (box2[2] - box2[0] + 1) * (box2[3] - box2[1] + 1)\n",
        "\n",
        "    # Calculate IoU\n",
        "    iou = intersection_area / float(box1_area + box2_area - intersection_area)\n",
        "    return iou\n",
        "\n",
        "# Function to evaluate the model\n",
        "@torch.no_grad()\n",
        "def evaluate(model, data_loader, device, iou_threshold=0.5):\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "\n",
        "    for images, targets in tqdm(data_loader, desc=\"Evaluating\"):\n",
        "        images = list(image.to(device) for image in images if image is not None)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets if t is not None]\n",
        "\n",
        "        predictions = model(images)\n",
        "\n",
        "        for i, prediction in enumerate(predictions):\n",
        "            confident_predictions = prediction['scores'] > 0.5\n",
        "            boxes = prediction['boxes'][confident_predictions].cpu().numpy()\n",
        "            labels = prediction['labels'][confident_predictions].cpu().numpy()\n",
        "            scores = prediction['scores'][confident_predictions].cpu().numpy()\n",
        "\n",
        "            image_predictions = []\n",
        "            for box, label, score in zip(boxes, labels, scores):\n",
        "                image_predictions.append({\n",
        "                    'bbox': box.tolist(),\n",
        "                    'category_id': label,\n",
        "                    'score': score\n",
        "                })\n",
        "\n",
        "            all_predictions.append({\n",
        "                'image_id': targets[i]['image_id'].item(),\n",
        "                'predictions': image_predictions\n",
        "            })\n",
        "\n",
        "    return all_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lo5yf1SNXFRa",
        "outputId": "4231970d-d896-4f6b-98d9-1c4ef442b55e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating:   4%|▎         | 10/271 [00:06<02:44,  1.59it/s]"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Calculate mAP\n",
        "def calculate_map(predictions, ground_truth, iou_threshold=0.5):\n",
        "    \"\"\"Calculates mean Average Precision (mAP) for a single class.\n",
        "    \"\"\"\n",
        "    all_gt_boxes = []\n",
        "    for gt in ground_truth:\n",
        "        for ann in gt['annotations']:\n",
        "            all_gt_boxes.append({\n",
        "                'image_id': gt['id'],\n",
        "                'bbox': ann['bbox'],\n",
        "                'category_id': ann['category_id']\n",
        "            })\n",
        "\n",
        "    # No need to iterate through categories, as we have only one\n",
        "    # Sort predictions by confidence score (descending)\n",
        "    sorted_predictions = sorted(\n",
        "        [p for pred in predictions for p in pred['predictions']],\n",
        "        key=lambda x: x['score'],\n",
        "        reverse=True\n",
        "    )\n",
        "\n",
        "    tp = np.zeros(len(sorted_predictions))\n",
        "    fp = np.zeros(len(sorted_predictions))\n",
        "    num_gt_boxes = len(all_gt_boxes)\n",
        "\n",
        "    # Calculate TP and FP for each prediction\n",
        "    for i, prediction in enumerate(sorted_predictions):\n",
        "        matched = False\n",
        "        for j, gt_box in enumerate(all_gt_boxes):\n",
        "            if prediction['image_id'] == gt_box['image_id'] and calculate_iou(prediction['bbox'], gt_box['bbox']) >= iou_threshold:\n",
        "                matched = True\n",
        "                all_gt_boxes.pop(j)  # Remove the matched ground truth box\n",
        "                break\n",
        "\n",
        "        if matched:\n",
        "            tp[i] = 1\n",
        "        else:\n",
        "            fp[i] = 1\n",
        "\n",
        "    # Calculate precision and recall\n",
        "    tp_cumulative = np.cumsum(tp)\n",
        "    fp_cumulative = np.cumsum(fp)\n",
        "    precision = tp_cumulative / (tp_cumulative + fp_cumulative + 1e-10)\n",
        "    recall = tp_cumulative / (num_gt_boxes + 1e-10)\n",
        "\n",
        "    # Calculate AP using the precision-recall curve\n",
        "    ap = calculate_ap_from_pr(precision, recall)\n",
        "    return ap\n",
        "\n",
        "def calculate_ap_from_pr(precision, recall):\n",
        "    \"\"\"Calculates Average Precision (AP) from precision and recall values.\n",
        "    \"\"\"\n",
        "    ap = 0.0\n",
        "    for i in range(len(recall) - 1):\n",
        "        ap += (recall[i + 1] - recall[i]) * precision[i + 1]\n",
        "    return ap\n",
        "\n",
        "    # Evaluation\n",
        "# Load the validation dataset\n",
        "val_dataset = ShipDataset(\"/content/dataset/ships-aerial-images/valid\", transforms=transform)\n",
        "val_data_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
        "\n",
        "# Evaluate the model\n",
        "predictions = evaluate(model, val_data_loader, device)\n",
        "\n",
        "# Load ground truth annotations for the validation set (in COCO format)\n",
        "with open('/content/dataset/ships-aerial-images/valid/annotations_coco/annotations_coco.json', 'r') as f:\n",
        "    gt_data = json.load(f)\n",
        "\n",
        "# Calculate and print mAP\n",
        "map_score = calculate_map(predictions, gt_data['images'])\n",
        "print(f\"mAP: {map_score:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
